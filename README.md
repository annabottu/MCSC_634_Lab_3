# MCSC_634_Lab_3

The purpose to this lab was to explore clustering techniques using the wine dataset that we got from sklearn library. This dataset consisted of 178 entries and 14 attributes which included alcohol, malic_acid, ash, alcalinity_of_ash, magnesium, total_phenols, flavanoids, nonflavanoid_phenols, proanthocyanins, color_intensity, hue, od280/od315_of_diluted_wines, proline, and target. We applied both K-Means and K-Medoids algorithms to the datsets, we also used Silhouetter Score and the Adjusted Rand Index to visualize the results to look at the structure of the cluster. From the results from the K-means clustering with k=3, the Silhouette Score was ~0.28. This score ranges from -1 to 1, where values closer to 1 indicate well-structured clusters, values near 0 suggest overlapping clusters, and negative values may indicate incorrect clustering. A score of 0.28 indicates that while the clusters are somewhat distinct, there is still some overlap between them. Additionally, the ARI ~0.90, which indicates a strong agreement between the predicted clusters and the actual wine classes, showing that the clustering closely aligns with the true labels. From the result from the K-Medoids clustering with k=3, the Silhouette score was ~0.27 so just like the K-mean clustering, it indicates that while the clusters are somewhat distinct, there is still some overlap between them. The ARI score was ~0.73, which means that the K-Medoids clusters are pretty aligned with the actual wine classes. After plotting both the K-Means and K-Medoids clusters side by side, we saw that the K-Means cluster produced a better defined cluster.The cluster for both the k-means and k-medoids look the same but there are less outliers for the k-means clusters the centroids are placed in the middle of the cluster. Where the K-medoids clusters look like they have a couple more outlers and the medoids are not centered like the k-means centroids. K-medoids can be good for data that has outliers, where k-means clustering worked best for this dataset. The one problem I came across was trying to find how to create these clusters since there are so many attributes, but after some online research I found that we can use Principal Component Anaylsis (PCA) which reduces the number of attributes (columns) to only keep the important ones. Other than that, everything else went smoothly.
